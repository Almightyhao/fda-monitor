import os
import json
import time
import urllib.parse
import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

# --- è¨­å®šå€ ---
EXCEL_PATH = os.path.join("public", "drugs.xlsx")
JSON_DB_PATH = os.path.join("public", "data.json")
BASE_URL = "https://mcp.fda.gov.tw"
MAX_CHAR_LIMIT = 15000  # é™åˆ¶æ¯å€‹è—¥å“æœ€å¤šå­˜ 1.5 è¬å­— (é›»å­ä»¿å–®é€šå¸¸ä¸æœƒè¶…éé€™æ•¸å­—)

def clean_text(text):
    """
    å¼·åŠ›æ¸…æ½”å·¥ï¼šåªä¿ç•™æœ‰æ„ç¾©çš„ä»¿å–®æ–‡å­—
    """
    if not text: return ""
    
    # 1. å°‡å¤šå€‹é€£çºŒæ›è¡Œè®Šç‚ºå–®ä¸€æ›è¡Œ
    text = re.sub(r'\n\s*\n', '\n', text)
    # 2. å»é™¤å¤šé¤˜çš„ç©ºç™½
    text = re.sub(r'[ \t]+', ' ', text)
    
    # 3. å¦‚æœæ–‡å­—é‚„æ˜¯å¤ªé•·ï¼Œå¼·åˆ¶æˆªæ–· (é˜²æ­¢æª”æ¡ˆçˆ†ç‚¸)
    if len(text) > MAX_CHAR_LIMIT:
        text = text[:MAX_CHAR_LIMIT] + "\n... (å…§å®¹éé•·å·²æˆªæ–·) ..."
        
    return text.strip()

def fetch_fda_html_only(license_id):
    """
    åªæŠ“å–é›»å­ä»¿å–® (HTML)
    """
    safe_license = urllib.parse.quote(license_id)
    url = f"{BASE_URL}/im_detail_1/{safe_license}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0"
    }
    
    print(f"    æª¢æŸ¥: {license_id} ...")
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        if res.status_code != 200:
            return f"é€£ç·šéŒ¯èª¤ (Code {res.status_code})"
            
        soup = BeautifulSoup(res.text, 'html.parser')

        # 1. ç§»é™¤ç¶²é ä¸Šçš„é›œè¨Š (å°è¦½åˆ—ã€é å°¾ã€è…³æœ¬ã€æ¨£å¼)
        # é€™ä¸€æ­¥éå¸¸é‡è¦ï¼Œèƒ½å¤§å¹…æ¸›å°‘æª”æ¡ˆå¤§å°
        for junk in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe", "svg"]):
            junk.extract()

        # 2. é–å®šå…§å®¹å€å¡Š
        # è¡›ç¦éƒ¨ç¶²ç«™é€šå¸¸å°‡å…§å®¹æ”¾åœ¨ class="im_detail_content" æˆ–é¡ä¼¼çš„å®¹å™¨ä¸­
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œæˆ‘å€‘å°±æŠ“ bodyï¼Œä½†å‰é¢å·²ç¶“æ¸…é™¤äº†å¤§éƒ¨åˆ†é›œè¨Š
        content_div = soup.find('div', class_='im_detail_content')
        
        if not content_div:
            # å˜—è©¦å¦ä¸€å€‹å¸¸è¦‹çš„å®¹å™¨ class (ä»¥é˜²æ”¹ç‰ˆ)
            content_div = soup.find('div', class_='container')
        
        # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°±ç”¨æ•´å€‹ body
        if not content_div:
            content_div = soup.body

        if not content_div:
            return "ç„¡æ³•è§£æç¶²é çµæ§‹"

        # 3. æå–æ–‡å­—
        page_text = content_div.get_text(separator='\n')
        
        # ==========================================
        # ğŸš¨ [æ–°å¢] åƒåœ¾é é¢éæ¿¾å™¨ ğŸš¨
        # å¦‚æœæŠ“åˆ°çš„æ–‡å­—åŒ…å«é€™äº›ã€Œæœå°‹é é¢ã€çš„ç‰¹å¾µè©ï¼Œä»£è¡¨é€£çµå¤±æ•ˆäº†ï¼Œè¢«å°å›é¦–é 
        # é€™æ¨£å°±ä¸æœƒå­˜å…¥ä¸€å †ã€Œé˜¿æ›¼ã€å·´è²å¤š...ã€çš„ç„¡ç”¨åœ‹å®¶åˆ—è¡¨
        if "è¥¿è—¥å“ä»¿å–®è³‡æ–™æŸ¥è©¢" in page_text and "è¨±å¯è­‰å­—è™ŸæŸ¥è©¢" in page_text:
            return "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™ (é€£çµå¤±æ•ˆæˆ–å·²ä¸‹æ¶)"
        # ==========================================
        
        # 4. é©—è­‰æ˜¯å¦çœŸçš„æœ‰ä»¿å–®å…§å®¹ (é¿å…æŠ“åˆ°å…¶ä»–ç¨®é¡çš„ç©ºé é¢)
        # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµå­—
        keywords = ["é©æ‡‰ç—‡", "ç”¨æ³•ç”¨é‡", "è­¦èª", "å‰¯ä½œç”¨", "ç¦å¿Œ", "äº¤äº’ä½œç”¨", "åŠ‘å‹"]
        hit_count = sum(1 for k in keywords if k in page_text)
        
        if hit_count >= 1:
            # æ˜¯ä¸€å€‹æœ‰æ•ˆçš„é›»å­ä»¿å–®
            return clean_text(page_text)
        else:
            return "æ­¤è—¥å“ç„¡é›»å­ä»¿å–®è³‡æ–™ (å¯èƒ½åƒ…æœ‰ PDF)"

    except Exception as e:
        return f"è®€å–å¤±æ•—: {str(e)}"

def main():
    print("=== é›»å­ä»¿å–®ç›£æ¸¬ç³»çµ± (HTML Only) ===")
    
    if not os.path.exists(EXCEL_PATH):
        print(f"æ‰¾ä¸åˆ° {EXCEL_PATH}")
        return

    try:
        df = pd.read_excel(EXCEL_PATH)
        df['è¨±å¯è­‰å­—è™Ÿ'] = df['è¨±å¯è­‰å­—è™Ÿ'].astype(str).str.strip()
    except Exception as e:
        print(f"Excel è®€å–å¤±æ•—: {e}")
        return

    # è®€å–èˆŠè³‡æ–™åº« (å¦‚æœæª”æ¡ˆå¤ªå¤§å£æ‰ï¼Œå°±é‡é–‹ä¸€å€‹)
    if os.path.exists(JSON_DB_PATH):
        try:
            with open(JSON_DB_PATH, 'r', encoding='utf-8') as f:
                db = json.load(f)
                old_items = {item['license']: item for item in db['items']}
        except:
            print("èˆŠè³‡æ–™åº«ææ¯€æˆ–æ ¼å¼ä¸ç¬¦ï¼Œå°‡å»ºç«‹æ–°è³‡æ–™åº«ã€‚")
            old_items = {}
    else:
        old_items = {}

    new_items_list = []

    for index, row in df.iterrows():
        lic_id = row['è¨±å¯è­‰å­—è™Ÿ']
        drug_name = row['è—¥å']
        drug_code = row['é™¢å…§ä»£ç¢¼']
        
        # åŸ·è¡Œæ–°çš„æŠ“å–é‚è¼¯
        current_text = fetch_fda_html_only(lic_id)
        
        old_record = old_items.get(lic_id, {})
        old_text = old_record.get('current_text', "")
        last_change = old_record.get('last_change_date', datetime.now().strftime('%Y-%m-%d'))
        
        is_changed = False
        # æ¯”å°é‚è¼¯ï¼šåªæœ‰ç•¶æ–°èˆŠéƒ½æœ‰æ–‡å­—ï¼Œä¸”ä¸ç›¸åŒæ™‚æ‰ç®—ç•°å‹•
        if old_text and current_text != old_text:
            # æ’é™¤æ‰ã€Œç„¡é›»å­ä»¿å–®ã€æˆ–ã€ŒæŸ¥ç„¡è³‡æ–™ã€é€™ç¨®ç³»çµ±è¨Šæ¯çš„è®Šå‹•
            # å¦‚æœå…©é‚Šéƒ½æ˜¯ç³»çµ±è¨Šæ¯ï¼Œå°±ä¸ç®—ç•°å‹•
            system_msgs = ["ç„¡é›»å­ä»¿å–®", "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™"]
            
            is_new_sys_msg = any(msg in current_text for msg in system_msgs)
            is_old_sys_msg = any(msg in old_text for msg in system_msgs)

            # å¦‚æœæ–°èˆŠæ–‡å­—åŒ…å«ç³»çµ±è¨Šæ¯ï¼Œæˆ‘å€‘ç¨å¾®æ”¾å¯¬æ¨™æº–
            # åªæœ‰ç•¶ã€ŒçœŸæ­£å…§å®¹ã€è®Šæˆã€Œç³»çµ±è¨Šæ¯ã€ï¼ˆä¸‹æ¶ï¼‰ï¼Œæˆ–ã€Œç³»çµ±è¨Šæ¯ã€è®Šæˆã€ŒçœŸæ­£å…§å®¹ã€ï¼ˆä¸Šæ¶ï¼‰æ‰ç®—
            # ä½†ç‚ºäº†ç°¡å–®èµ·è¦‹ï¼Œåªè¦æ–‡å­—ä¸åŒï¼Œä¸”ä¸æ˜¯ç´”ç²¹çš„æ ¼å¼å·®ç•°ï¼Œå°±ç®—ç•°å‹•
            # é€™è£¡æˆ‘å€‘ç¶­æŒæ‚¨åŸæœ¬çš„é‚è¼¯ï¼Œä½†åŠ ä¸Šå°æ–°è¨Šæ¯çš„æ’é™¤
            
            if not (is_new_sys_msg and is_old_sys_msg):
                 is_changed = True
                 last_change = datetime.now().strftime('%Y-%m-%d')
                 print(f"    [!] ç™¼ç¾ç•°å‹•: {drug_name}")
        
        if not old_text:
            old_text = current_text 

        new_items_list.append({
            "code": drug_code,
            "name": drug_name,
            "license": lic_id,
            "fda_url": f"{BASE_URL}/im_detail_1/{urllib.parse.quote(lic_id)}",
            "old_text": old_text if is_changed else current_text,
            "current_text": current_text,
            "is_changed": is_changed,
            "last_change_date": last_change
        })
        
        # ç¨å¾®æš«åœä¸€ä¸‹ï¼Œå°ä¼ºæœå™¨æº«æŸ”ä¸€é»
        time.sleep(0.5)

    final_data = {
        "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "items": new_items_list
    }
    
    # æª¢æŸ¥æœ€çµ‚æª”æ¡ˆå¤§å°
    json_str = json.dumps(final_data, ensure_ascii=False, indent=2)
    print(f"è³‡æ–™åº«å¤§å°é ä¼°: {len(json_str)/1024/1024:.2f} MB")

    with open(JSON_DB_PATH, 'w', encoding='utf-8') as f:
        f.write(json_str)
        
    print(f"æ›´æ–°å®Œæˆ")

if __name__ == "__main__":
    main()

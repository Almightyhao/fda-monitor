import os
import json
import time
import urllib.parse
import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

# --- è¨­å®šå€ ---
EXCEL_PATH = os.path.join("public", "drugs.xlsx")
JSON_DB_PATH = os.path.join("public", "data.json")
BASE_URL = "https://mcp.fda.gov.tw"

# âœ… å­—æ•¸é™åˆ¶ï¼šæ—¢ç„¶æˆ‘å€‘å·²ç¶“åˆ‡æ‰æœ€ä½”ç©ºé–“çš„è‡¨åºŠè³‡æ–™ï¼Œ
# å‰©ä¸‹çš„ã€Œé©æ‡‰ç—‡ã€å‰¯ä½œç”¨ã€é€šå¸¸ä¸æœƒè¶…é 1.5 è¬å­—ï¼Œé€™è£¡è¨­å€‹ 20000 ç•¶ä½œæœ€å¾Œä¸€é“é˜²ç·šå³å¯ã€‚
MAX_CHAR_LIMIT = 20000 

def clean_text(text):
    """
    å¼·åŠ›æ¸…æ½”å·¥ï¼šåªä¿ç•™æœ‰æ„ç¾©çš„ä»¿å–®æ–‡å­—ï¼Œä¸¦åˆ‡é™¤å¾Œæ®µå­¸è¡“è³‡æ–™
    """
    if not text: return ""
    
    # 1. å°‡å¤šå€‹é€£çºŒæ›è¡Œè®Šç‚ºå–®ä¸€æ›è¡Œ
    text = re.sub(r'\n\s*\n', '\n', text)
    # 2. å»é™¤å¤šé¤˜çš„ç©ºç™½
    text = re.sub(r'[ \t]+', ' ', text)
    
    # ==========================================
    # âœ‚ï¸ [æ–°å¢] æ‰‹è¡“åˆ€åˆ‡é™¤æ³•ï¼šæ’é™¤å¾Œæ®µå­¸è¡“è³‡æ–™ âœ‚ï¸
    # ==========================================
    # é€™äº›é—œéµå­—é€šå¸¸å‡ºç¾åœ¨ä»¿å–®çš„å¾Œå¤§åŠæ®µï¼Œæˆ‘å€‘ä¸€çœ‹åˆ°å°±åˆ‡æ–·
    # åŒ…å«å…¨å½¢æ•¸å­—ã€åŠå½¢æ•¸å­—ã€æˆ–ç´”æ–‡å­—æ¨™é¡Œï¼Œç›¡é‡æ¶µè“‹å„ç¨®å¯«æ³•
    cut_off_keywords = [
        "10 è—¥ç†ç‰¹æ€§", "10.è—¥ç†ç‰¹æ€§", "10. è—¥ç†ç‰¹æ€§", "æ‹¾ã€è—¥ç†ç‰¹æ€§",
        "11 è—¥ç‰©å‹•åŠ›å­¸", "11.è—¥ç‰©å‹•åŠ›å­¸", "11. è—¥ç‰©å‹•åŠ›å­¸", "æ‹¾å£¹ã€è—¥ç‰©å‹•åŠ›å­¸",
        "12 è‡¨åºŠè©¦é©—", "12.è‡¨åºŠè©¦é©—", "12. è‡¨åºŠè©¦é©—", "æ‹¾è²³ã€è‡¨åºŠè©¦é©—",
        "è—¥ç†ç‰¹æ€§", "è—¥ç‰©å‹•åŠ›å­¸ç‰¹æ€§", "è‡¨åºŠè©¦é©—è³‡æ–™" # æœ€å¾Œç”¨ç´”é—œéµå­—å…œåº•
    ]
    
    # å°‹æ‰¾é€™äº›é—œéµå­—ä¸­ï¼Œæœ€æ—©å‡ºç¾çš„ä½ç½®
    earliest_cut_index = -1
    cut_reason = ""
    
    for keyword in cut_off_keywords:
        idx = text.find(keyword)
        # å¦‚æœæ‰¾åˆ°äº†ï¼Œä¸”æ¯”ç›®å‰æ‰¾åˆ°çš„æ›´å‰é¢ (æˆ–é‚„æ²’æ‰¾åˆ°é)
        if idx != -1:
            # ç¢ºä¿ä¸æ˜¯åœ¨æ–‡ç« é–‹é ­å°±è¢«åˆ‡æ‰ (ä¾‹å¦‚ç›®éŒ„å€)ï¼Œæˆ‘å€‘å‡è¨­é€™äº›ç« ç¯€è‡³å°‘åœ¨ 500 å­—ä»¥å¾Œ
            if idx > 100: 
                if earliest_cut_index == -1 or idx < earliest_cut_index:
                    earliest_cut_index = idx
                    cut_reason = keyword

    # å¦‚æœæœ‰æ‰¾åˆ°åˆ‡é»ï¼Œå°±åŸ·è¡Œåˆ‡é™¤
    if earliest_cut_index != -1:
        text = text[:earliest_cut_index]
        text += f"\n\n--- (å·²çœç•¥ã€Œ{cut_reason}ã€åŠå¾ŒçºŒè©³ç´°è³‡æ–™ä»¥ç¯€çœç©ºé–“) ---"

    # ==========================================
    
    # 3. æœ€å¾Œé˜²ç·šï¼šå¦‚æœåˆ‡å®Œé‚„æ˜¯å¤ªé•· (æ¥µå°‘è¦‹)ï¼Œå†å¼·åˆ¶æˆªæ–·
    if len(text) > MAX_CHAR_LIMIT:
        text = text[:MAX_CHAR_LIMIT] + f"\n... (å…§å®¹éé•·ï¼Œåƒ…é¡¯ç¤ºå‰ {MAX_CHAR_LIMIT} å­—) ..."
        
    return text.strip()

def fetch_fda_html_only(license_id):
    """
    åªæŠ“å–é›»å­ä»¿å–® (HTML)
    """
    safe_license = urllib.parse.quote(license_id)
    url = f"{BASE_URL}/im_detail_1/{safe_license}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0"
    }
    
    print(f"    æª¢æŸ¥: {license_id} ...")
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        if res.status_code != 200:
            return f"é€£ç·šéŒ¯èª¤ (Code {res.status_code})"
            
        soup = BeautifulSoup(res.text, 'html.parser')

        # 1. ç§»é™¤ç¶²é ä¸Šçš„é›œè¨Š
        for junk in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe", "svg"]):
            junk.extract()

        # 2. é–å®šå…§å®¹å€å¡Š
        content_div = soup.find('div', class_='im_detail_content')
        if not content_div:
            content_div = soup.find('div', class_='container')
        if not content_div:
            content_div = soup.body

        if not content_div:
            return "ç„¡æ³•è§£æç¶²é çµæ§‹"

        # 3. æå–æ–‡å­—
        page_text = content_div.get_text(separator='\n')
        
        # ğŸš¨ åƒåœ¾é é¢éæ¿¾å™¨
        if "è¥¿è—¥å“ä»¿å–®è³‡æ–™æŸ¥è©¢" in page_text and "è¨±å¯è­‰å­—è™ŸæŸ¥è©¢" in page_text:
            return "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™ (é€£çµå¤±æ•ˆæˆ–å·²ä¸‹æ¶)"
        
        # 4. é©—è­‰æ˜¯å¦çœŸçš„æœ‰ä»¿å–®å…§å®¹
        keywords = ["é©æ‡‰ç—‡", "ç”¨æ³•ç”¨é‡", "è­¦èª", "å‰¯ä½œç”¨", "ç¦å¿Œ", "äº¤äº’ä½œç”¨", "åŠ‘å‹"]
        hit_count = sum(1 for k in keywords if k in page_text)
        
        if hit_count >= 1:
            return clean_text(page_text)
        else:
            return "æ­¤è—¥å“ç„¡é›»å­ä»¿å–®è³‡æ–™ (å¯èƒ½åƒ…æœ‰ PDF)"

    except Exception as e:
        return f"è®€å–å¤±æ•—: {str(e)}"

def main():
    print("=== é›»å­ä»¿å–®ç›£æ¸¬ç³»çµ± (Extreme Save Mode) ===")
    
    if not os.path.exists(EXCEL_PATH):
        print(f"æ‰¾ä¸åˆ° {EXCEL_PATH}")
        return

    try:
        df = pd.read_excel(EXCEL_PATH)
        df['è¨±å¯è­‰å­—è™Ÿ'] = df['è¨±å¯è­‰å­—è™Ÿ'].astype(str).str.strip()
    except Exception as e:
        print(f"Excel è®€å–å¤±æ•—: {e}")
        return

    # è®€å–èˆŠè³‡æ–™åº«
    if os.path.exists(JSON_DB_PATH):
        try:
            with open(JSON_DB_PATH, 'r', encoding='utf-8') as f:
                db = json.load(f)
                old_items = {item['license']: item for item in db['items']}
        except:
            print("èˆŠè³‡æ–™åº«ææ¯€ï¼Œå°‡å»ºç«‹æ–°è³‡æ–™åº«ã€‚")
            old_items = {}
    else:
        old_items = {}

    new_items_list = []

    for index, row in df.iterrows():
        lic_id = row['è¨±å¯è­‰å­—è™Ÿ']
        drug_name = row['è—¥å']
        drug_code = row['é™¢å…§ä»£ç¢¼']
        
        current_text = fetch_fda_html_only(lic_id)
        
        old_record = old_items.get(lic_id, {})
        
        # é‚„åŸèˆŠè³‡æ–™é‚è¼¯ (å°æ‡‰ä¸Šæ¬¡çš„çœç©ºé–“é‚è¼¯)
        saved_old_text = old_record.get('old_text', "")
        if not saved_old_text:
             saved_old_text = old_record.get('current_text', "")

        last_change = old_record.get('last_change_date', datetime.now().strftime('%Y-%m-%d'))
        
        is_changed = False
        
        if saved_old_text and current_text != saved_old_text:
            system_msgs = ["ç„¡é›»å­ä»¿å–®", "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™"]
            is_new_sys_msg = any(msg in current_text for msg in system_msgs)
            is_old_sys_msg = any(msg in saved_old_text for msg in system_msgs)
            
            if not (is_new_sys_msg and is_old_sys_msg):
                 is_changed = True
                 last_change = datetime.now().strftime('%Y-%m-%d')
                 print(f"    [!] ç™¼ç¾ç•°å‹•: {drug_name}")
        
        if not saved_old_text:
            saved_old_text = current_text 

        new_items_list.append({
            "code": drug_code,
            "name": drug_name,
            "license": lic_id,
            "fda_url": f"{BASE_URL}/im_detail_1/{urllib.parse.quote(lic_id)}",
            # åªåœ¨ç•°å‹•æ™‚å­˜èˆŠè³‡æ–™
            "old_text": saved_old_text if is_changed else "", 
            "current_text": current_text,
            "is_changed": is_changed,
            "last_change_date": last_change
        })
        
        time.sleep(0.5)

    final_data = {
        "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "items": new_items_list
    }
    
    json_str = json.dumps(final_data, ensure_ascii=False, indent=2)
    print(f"è³‡æ–™åº«å¤§å°é ä¼°: {len(json_str)/1024/1024:.2f} MB")

    with open(JSON_DB_PATH, 'w', encoding='utf-8') as f:
        f.write(json_str)
        
    print(f"æ›´æ–°å®Œæˆ")

if __name__ == "__main__":
    main()

import os
import json
import time
import urllib.parse
import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

# --- è¨­å®šå€ ---
EXCEL_PATH = os.path.join("public", "drugs.xlsx")
JSON_DB_PATH = os.path.join("public", "data.json")
BASE_URL = "https://mcp.fda.gov.tw"

# âœ… æ”¾å¯¬é™åˆ¶ï¼šæ”¹æˆ 3 è¬å­—ï¼Œè®“æ‚¨èƒ½çœ‹åˆ°å®Œæ•´å…§å®¹
# é€éä¸‹é¢çš„ã€Œç©ºé–“ç¯€çœé‚è¼¯ã€ï¼Œæˆ‘å€‘æœ‰æœ¬éŒ¢å­˜é€™éº¼å¤šå­—ï¼
MAX_CHAR_LIMIT = 30000 

def clean_text(text):
    """
    å¼·åŠ›æ¸…æ½”å·¥ï¼šåªä¿ç•™æœ‰æ„ç¾©çš„ä»¿å–®æ–‡å­—
    """
    if not text: return ""
    
    # 1. å°‡å¤šå€‹é€£çºŒæ›è¡Œè®Šç‚ºå–®ä¸€æ›è¡Œ
    text = re.sub(r'\n\s*\n', '\n', text)
    # 2. å»é™¤å¤šé¤˜çš„ç©ºç™½
    text = re.sub(r'[ \t]+', ' ', text)
    
    # 3. å®‰å…¨é–¥ï¼šé›–ç„¶æ”¾å¯¬äº†ï¼Œé‚„æ˜¯è¦é˜²ç¯„é‚£ç¨® 100 è¬å­—çš„ç•°å¸¸è³‡æ–™
    if len(text) > MAX_CHAR_LIMIT:
        text = text[:MAX_CHAR_LIMIT] + f"\n... (å…§å®¹éé•·ï¼Œåƒ…é¡¯ç¤ºå‰ {MAX_CHAR_LIMIT} å­—) ..."
        
    return text.strip()

def fetch_fda_html_only(license_id):
    """
    åªæŠ“å–é›»å­ä»¿å–® (HTML)
    """
    safe_license = urllib.parse.quote(license_id)
    url = f"{BASE_URL}/im_detail_1/{safe_license}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0"
    }
    
    print(f"    æª¢æŸ¥: {license_id} ...")
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        if res.status_code != 200:
            return f"é€£ç·šéŒ¯èª¤ (Code {res.status_code})"
            
        soup = BeautifulSoup(res.text, 'html.parser')

        # 1. ç§»é™¤ç¶²é ä¸Šçš„é›œè¨Š
        for junk in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe", "svg"]):
            junk.extract()

        # 2. é–å®šå…§å®¹å€å¡Š
        content_div = soup.find('div', class_='im_detail_content')
        if not content_div:
            content_div = soup.find('div', class_='container')
        if not content_div:
            content_div = soup.body

        if not content_div:
            return "ç„¡æ³•è§£æç¶²é çµæ§‹"

        # 3. æå–æ–‡å­—
        page_text = content_div.get_text(separator='\n')
        
        # ğŸš¨ åƒåœ¾é é¢éæ¿¾å™¨ (ä¿ç•™é€™å€‹åŠŸèƒ½ï¼Œé€™ä¹Ÿæ˜¯çœç©ºé–“çš„é—œéµ)
        if "è¥¿è—¥å“ä»¿å–®è³‡æ–™æŸ¥è©¢" in page_text and "è¨±å¯è­‰å­—è™ŸæŸ¥è©¢" in page_text:
            return "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™ (é€£çµå¤±æ•ˆæˆ–å·²ä¸‹æ¶)"
        
        # 4. é©—è­‰æ˜¯å¦çœŸçš„æœ‰ä»¿å–®å…§å®¹
        keywords = ["é©æ‡‰ç—‡", "ç”¨æ³•ç”¨é‡", "è­¦èª", "å‰¯ä½œç”¨", "ç¦å¿Œ", "äº¤äº’ä½œç”¨", "åŠ‘å‹"]
        hit_count = sum(1 for k in keywords if k in page_text)
        
        if hit_count >= 1:
            return clean_text(page_text)
        else:
            return "æ­¤è—¥å“ç„¡é›»å­ä»¿å–®è³‡æ–™ (å¯èƒ½åƒ…æœ‰ PDF)"

    except Exception as e:
        return f"è®€å–å¤±æ•—: {str(e)}"

def main():
    print("=== é›»å­ä»¿å–®ç›£æ¸¬ç³»çµ± (Smart Save Mode) ===")
    
    if not os.path.exists(EXCEL_PATH):
        print(f"æ‰¾ä¸åˆ° {EXCEL_PATH}")
        return

    try:
        df = pd.read_excel(EXCEL_PATH)
        df['è¨±å¯è­‰å­—è™Ÿ'] = df['è¨±å¯è­‰å­—è™Ÿ'].astype(str).str.strip()
    except Exception as e:
        print(f"Excel è®€å–å¤±æ•—: {e}")
        return

    # è®€å–èˆŠè³‡æ–™åº«
    if os.path.exists(JSON_DB_PATH):
        try:
            with open(JSON_DB_PATH, 'r', encoding='utf-8') as f:
                db = json.load(f)
                old_items = {item['license']: item for item in db['items']}
        except:
            print("èˆŠè³‡æ–™åº«ææ¯€ï¼Œå°‡å»ºç«‹æ–°è³‡æ–™åº«ã€‚")
            old_items = {}
    else:
        old_items = {}

    new_items_list = []

    for index, row in df.iterrows():
        lic_id = row['è¨±å¯è­‰å­—è™Ÿ']
        drug_name = row['è—¥å']
        drug_code = row['é™¢å…§ä»£ç¢¼']
        
        # åŸ·è¡Œæ–°çš„æŠ“å–é‚è¼¯
        current_text = fetch_fda_html_only(lic_id)
        
        old_record = old_items.get(lic_id, {})
        
        # ğŸ’¡ [é—œéµé‚è¼¯] é‚„åŸèˆŠè³‡æ–™
        # å¦‚æœè³‡æ–™åº«è£¡çš„ old_text æ˜¯ç©ºçš„ (å› ç‚ºä¸Šæ¬¡ç‚ºäº†çœç©ºé–“æ²’å­˜)ï¼Œ
        # ä»£è¡¨ä¸Šæ¬¡æ²’æœ‰ç•°å‹•ï¼Œæ‰€ä»¥ã€ŒèˆŠçš„ old_textã€å…¶å¯¦å°±æ˜¯ã€Œè³‡æ–™åº«è£¡çš„ current_textã€ã€‚
        saved_old_text = old_record.get('old_text', "")
        if not saved_old_text:
             saved_old_text = old_record.get('current_text', "")

        last_change = old_record.get('last_change_date', datetime.now().strftime('%Y-%m-%d'))
        
        is_changed = False
        
        # æ¯”å°é‚è¼¯
        if saved_old_text and current_text != saved_old_text:
            system_msgs = ["ç„¡é›»å­ä»¿å–®", "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™"]
            is_new_sys_msg = any(msg in current_text for msg in system_msgs)
            is_old_sys_msg = any(msg in saved_old_text for msg in system_msgs)
            
            if not (is_new_sys_msg and is_old_sys_msg):
                 is_changed = True
                 last_change = datetime.now().strftime('%Y-%m-%d')
                 print(f"    [!] ç™¼ç¾ç•°å‹•: {drug_name}")
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼ŒæŠŠèˆŠè³‡æ–™è¨­ç‚ºè·Ÿæ–°çš„ä¸€æ¨£
        if not saved_old_text:
            saved_old_text = current_text 

        # ==========================================
        # ğŸš¨ [æ ¸å¿ƒä¿®æ­£] æ™ºæ…§çœç©ºé–“é‚è¼¯ ğŸš¨
        # 1. åªæœ‰ç•¶ã€Œis_changed ç‚º Trueã€æ™‚ï¼Œæˆ‘å€‘æ‰å­˜ old_textã€‚
        # 2. å¦‚æœæ²’ç•°å‹•ï¼Œold_text å­˜æˆç©ºå­—ä¸² ""ã€‚
        # 3. é€™æ¨£å¯ä»¥ç¯€çœ 50% çš„ç©ºé–“ï¼Œè®“æˆ‘å€‘å¯ä»¥æ”¾å¿ƒåœ°æŠŠå­—æ•¸é™åˆ¶èª¿å¤§ï¼
        # ==========================================
        new_items_list.append({
            "code": drug_code,
            "name": drug_name,
            "license": lic_id,
            "fda_url": f"{BASE_URL}/im_detail_1/{urllib.parse.quote(lic_id)}",
            
            "old_text": saved_old_text if is_changed else "", # âœ… çœç©ºé–“é—œéµ
            "current_text": current_text,
            
            "is_changed": is_changed,
            "last_change_date": last_change
        })
        
        time.sleep(0.5)

    final_data = {
        "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "items": new_items_list
    }
    
    json_str = json.dumps(final_data, ensure_ascii=False, indent=2)
    print(f"è³‡æ–™åº«å¤§å°é ä¼°: {len(json_str)/1024/1024:.2f} MB")

    with open(JSON_DB_PATH, 'w', encoding='utf-8') as f:
        f.write(json_str)
        
    print(f"æ›´æ–°å®Œæˆ")

if __name__ == "__main__":
    main()

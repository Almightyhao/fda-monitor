import os
import json
import time
import urllib.parse
import pandas as pd
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

# --- è¨­å®šå€ ---
EXCEL_PATH = os.path.join("public", "drugs.xlsx")
JSON_DB_PATH = os.path.join("public", "data.json")
BASE_URL = "https://mcp.fda.gov.tw"

# âœ… å­—æ•¸é™åˆ¶ï¼šå·²åˆ‡é™¤ä¸­é–“æœ€ä½”ç©ºé–“çš„è‡¨åºŠè³‡æ–™ï¼Œå‰©ä¸‹å…§å®¹çµ¦ 20000 å­—å¾ˆå®‰å…¨
MAX_CHAR_LIMIT = 20000 

def clean_text(text):
    """
    å¼·åŠ›æ¸…æ½”å·¥ï¼šæ™ºæ…§æŒ–ç©ºæ¨¡å¼
    é‚è¼¯ï¼šåˆ‡é™¤ [10~12 ç« ç¯€] (è—¥ç†/å‹•åŠ›å­¸/è‡¨åºŠ)ï¼Œä½†ä¿ç•™ [13~15 ç« ç¯€] (åŒ…è£/ç—…äººé ˆçŸ¥)
    """
    if not text: return ""
    
    # 1. åŸºç¤æ¸…ç†
    text = re.sub(r'\n\s*\n', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    # ==========================================
    # âœ‚ï¸ [æŒ–ç©ºæ‰‹è¡“] è¨­å®šåˆ‡é™¤çš„ã€Œèµ·é»ã€èˆ‡ã€Œçµ‚é»ã€
    # ==========================================
    
    # èµ·é»ï¼šçœ‹åˆ°é€™äº›ç« ç¯€é–‹å§‹åˆ‡ (10, 11, 12)
    start_keywords = [
        "10 è—¥ç†ç‰¹æ€§", "10.è—¥ç†ç‰¹æ€§", "10. è—¥ç†ç‰¹æ€§", "10.0 è—¥ç†ç‰¹æ€§", "æ‹¾ã€è—¥ç†ç‰¹æ€§",
        "11 è—¥ç‰©å‹•åŠ›å­¸", "11.è—¥ç‰©å‹•åŠ›å­¸", "11. è—¥ç‰©å‹•åŠ›å­¸", "11.0 è—¥ç‰©å‹•åŠ›å­¸", "æ‹¾å£¹ã€è—¥ç‰©å‹•åŠ›å­¸",
        "12 è‡¨åºŠè©¦é©—", "12.è‡¨åºŠè©¦é©—", "12. è‡¨åºŠè©¦é©—", "12.0 è‡¨åºŠè©¦é©—", "æ‹¾è²³ã€è‡¨åºŠè©¦é©—"
    ]
    
    # çµ‚é»ï¼šçœ‹åˆ°é€™äº›ç« ç¯€è¦æ¥å›ä¾† (13, 14, 15)
    end_keywords = [
        "13 åŒ…è£", "13.åŒ…è£", "13. åŒ…è£", "13.0 åŒ…è£", "æ‹¾åƒã€åŒ…è£",
        "14 ç—…äºº", "14.ç—…äºº", "14. ç—…äºº", "14.0 ç—…äºº", "æ‹¾è‚†ã€ç—…äºº",
        "15 å…¶ä»–", "15.å…¶ä»–", "15. å…¶ä»–", "15.0 å…¶ä»–", "æ‹¾ä¼ã€å…¶ä»–"
    ]
    
    # --- æ­¥é©Ÿ A: å°‹æ‰¾åˆ‡é™¤èµ·é» ---
    start_idx = -1
    for kw in start_keywords:
        idx = text.find(kw)
        if idx != -1 and idx > 100: # é¿é–‹ç›®éŒ„å€
            if start_idx == -1 or idx < start_idx:
                start_idx = idx

    # --- æ­¥é©Ÿ B: åŸ·è¡ŒæŒ–ç©º ---
    if start_idx != -1:
        # å°‹æ‰¾ã€Œèµ·é»ä¹‹å¾Œã€æœ€æ—©å‡ºç¾çš„çµ‚é»
        end_idx = -1
        for kw in end_keywords:
            idx = text.find(kw, start_idx) # åªå¾ start_idx ä¹‹å¾Œæ‰¾
            if idx != -1:
                if end_idx == -1 or idx < end_idx:
                    end_idx = idx
        
        # ç‹€æ³ 1: æœ‰æ‰¾åˆ°çµ‚é» (ä¿ç•™å¾Œæ®µ)
        if end_idx != -1:
            part_1 = text[:start_idx]
            part_2 = text[end_idx:]
            text = f"{part_1}\n\n--- (å·²çœç•¥ 10~12 ç« ç¯€ä¹‹å­¸è¡“è³‡æ–™) ---\n\n{part_2}"
            
        # ç‹€æ³ 2: æ²’æ‰¾åˆ°çµ‚é» (å¾Œæ®µå…¨åˆ‡)
        else:
            text = text[:start_idx]
            text += "\n\n--- (å·²çœç•¥å¾ŒçºŒå­¸è¡“åŠè‡¨åºŠè³‡æ–™) ---"

    # æœ€å¾Œé˜²ç·š
    if len(text) > MAX_CHAR_LIMIT:
        text = text[:MAX_CHAR_LIMIT] + f"\n... (å…§å®¹éé•·ï¼Œåƒ…é¡¯ç¤ºå‰ {MAX_CHAR_LIMIT} å­—) ..."
        
    return text.strip()

def fetch_fda_html_only(license_id):
    """
    åªæŠ“å–é›»å­ä»¿å–® (HTML) + åƒåœ¾éæ¿¾
    """
    safe_license = urllib.parse.quote(license_id)
    url = f"{BASE_URL}/im_detail_1/{safe_license}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0"
    }
    
    print(f"    æª¢æŸ¥: {license_id} ...")
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        if res.status_code != 200:
            return f"é€£ç·šéŒ¯èª¤ (Code {res.status_code})"
            
        soup = BeautifulSoup(res.text, 'html.parser')

        for junk in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe", "svg"]):
            junk.extract()

        content_div = soup.find('div', class_='im_detail_content')
        if not content_div:
            content_div = soup.find('div', class_='container')
        if not content_div:
            content_div = soup.body

        if not content_div:
            return "ç„¡æ³•è§£æç¶²é çµæ§‹"

        page_text = content_div.get_text(separator='\n')
        
        # ğŸš¨ åƒåœ¾é é¢éæ¿¾å™¨
        if "è¥¿è—¥å“ä»¿å–®è³‡æ–™æŸ¥è©¢" in page_text and "è¨±å¯è­‰å­—è™ŸæŸ¥è©¢" in page_text:
            return "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™ (é€£çµå¤±æ•ˆæˆ–å·²ä¸‹æ¶)"
        
        keywords = ["é©æ‡‰ç—‡", "ç”¨æ³•ç”¨é‡", "è­¦èª", "å‰¯ä½œç”¨", "ç¦å¿Œ", "äº¤äº’ä½œç”¨", "åŠ‘å‹"]
        hit_count = sum(1 for k in keywords if k in page_text)
        
        if hit_count >= 1:
            return clean_text(page_text)
        else:
            return "æ­¤è—¥å“ç„¡é›»å­ä»¿å–®è³‡æ–™ (å¯èƒ½åƒ…æœ‰ PDF)"

    except Exception as e:
        return f"è®€å–å¤±æ•—: {str(e)}"

def main():
    print("=== é›»å­ä»¿å–®ç›£æ¸¬ç³»çµ± (Logic Fixed + Hollow Mode) ===")
    
    if not os.path.exists(EXCEL_PATH):
        print(f"æ‰¾ä¸åˆ° {EXCEL_PATH}")
        return

    try:
        df = pd.read_excel(EXCEL_PATH)
        df['è¨±å¯è­‰å­—è™Ÿ'] = df['è¨±å¯è­‰å­—è™Ÿ'].astype(str).str.strip()
    except Exception as e:
        print(f"Excel è®€å–å¤±æ•—: {e}")
        return

    # è®€å–èˆŠè³‡æ–™åº«
    if os.path.exists(JSON_DB_PATH):
        try:
            with open(JSON_DB_PATH, 'r', encoding='utf-8') as f:
                db = json.load(f)
                old_items = {item['license']: item for item in db['items']}
        except:
            print("èˆŠè³‡æ–™åº«ææ¯€ï¼Œå°‡å»ºç«‹æ–°è³‡æ–™åº«ã€‚")
            old_items = {}
    else:
        old_items = {}

    new_items_list = []

    for index, row in df.iterrows():
        lic_id = row['è¨±å¯è­‰å­—è™Ÿ']
        drug_name = row['è—¥å']
        drug_code = row['é™¢å…§ä»£ç¢¼']
        
        current_text = fetch_fda_html_only(lic_id)
        
        old_record = old_items.get(lic_id, {})
        
        # ğŸŸ¢ [é—œéµä¿®æ­£] 
        # æˆ‘å€‘å¿…é ˆè·Ÿä¸Šæ¬¡çš„ "current_text" æ¯”å°ï¼Œè€Œä¸æ˜¯ "old_text"
        # é€™æ¨£æ‰èƒ½æ“ºè„«èˆŠè³‡æ–™åº«è£¡é¢çš„åƒåœ¾æ­·å²
        previous_text = old_record.get('current_text', "")
        last_change = old_record.get('last_change_date', datetime.now().strftime('%Y-%m-%d'))
        
        is_changed = False
        
        # æ¯”å°é‚è¼¯
        if previous_text and current_text != previous_text:
            # æ’é™¤ç³»çµ±è¨Šæ¯çš„è®Šå‹•
            system_msgs = ["ç„¡é›»å­ä»¿å–®", "æŸ¥ç„¡é›»å­ä»¿å–®è³‡æ–™"]
            is_new_sys_msg = any(msg in current_text for msg in system_msgs)
            is_old_sys_msg = any(msg in previous_text for msg in system_msgs)
            
            if not (is_new_sys_msg and is_old_sys_msg):
                 is_changed = True
                 last_change = datetime.now().strftime('%Y-%m-%d')
                 print(f"    [!] ç™¼ç¾ç•°å‹•: {drug_name}")
        
        # æ±ºå®šè¦å­˜ä»€éº¼ç•¶ old_text
        # æœ‰ç•°å‹• -> å­˜ previous_text (ç‚ºäº†é¡¯ç¤ºæ¯”å°)
        # æ²’ç•°å‹• -> å­˜ "" (ç‚ºäº†çœç©ºé–“)
        final_old_text = previous_text if is_changed else ""

        new_items_list.append({
            "code": drug_code,
            "name": drug_name,
            "license": lic_id,
            "fda_url": f"{BASE_URL}/im_detail_1/{urllib.parse.quote(lic_id)}",
            "old_text": final_old_text, 
            "current_text": current_text,
            "is_changed": is_changed,
            "last_change_date": last_change
        })
        
        time.sleep(0.5)

    final_data = {
        "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "items": new_items_list
    }
    
    json_str = json.dumps(final_data, ensure_ascii=False, indent=2)
    print(f"è³‡æ–™åº«å¤§å°é ä¼°: {len(json_str)/1024/1024:.2f} MB")

    with open(JSON_DB_PATH, 'w', encoding='utf-8') as f:
        f.write(json_str)
        
    print(f"æ›´æ–°å®Œæˆ")

if __name__ == "__main__":
    main()
